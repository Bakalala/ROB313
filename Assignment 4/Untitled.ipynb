{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "from data.data_utils import load_dataset\n",
    "\n",
    "\n",
    "__author__ = 'Christopher Agia (1003243509)'\n",
    "__date__ = 'April 2, 2019'\n",
    "\n",
    "\n",
    "# Useful Utility Functions\n",
    "def compute_accuracy_ratio(y_test, y_estimates):\n",
    "    return (y_estimates == y_test).sum() / len(y_test)\n",
    "\n",
    "\n",
    "def X_mat(x_data):\n",
    "    X = np.ones((len(x_data), len(x_data[0]) + 1))\n",
    "    X[:, 1:] = x_data\n",
    "    return X\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return np.divide(1, np.add(1, np.exp(-1*z)))\n",
    "\n",
    "\n",
    "def log_likelihood(x_prod, y_act):\n",
    "    log_p = np.dot(y_act.T, np.log(sigmoid(x_prod))) + np.dot(np.subtract(1, y_act).T, np.log(np.subtract(1, sigmoid(x_prod))))\n",
    "    return log_p\n",
    "\n",
    "\n",
    "def likelihood_grad(X, x_prod, y_act):\n",
    "    grad = np.zeros(np.shape(X[0]))\n",
    "    for i in range(len(x_prod)):\n",
    "        grad += (y_act[i] - sigmoid(x_prod[i])) * X[i]\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_prior(w, sigma):\n",
    "    return -len(w)/2 * np.log(2 * np.pi) - len(w)/2 * np.log(sigma) - 1/(2 * sigma) * np.dot(w.T, w)\n",
    "\n",
    "\n",
    "def prior_grad(w, sigma):\n",
    "    return -1/sigma * w\n",
    "\n",
    "\n",
    "def prior_2grad(w, sigma):\n",
    "    return -1/sigma * np.eye(len(w))\n",
    "\n",
    "\n",
    "def log_g(hessian):\n",
    "    return 1/2 * np.log(np.linalg.det(-1 * hessian)) - len(hessian) / 2 * np.log(2 * np.pi)\n",
    "\n",
    "\n",
    "def likelihood(x, y):\n",
    "    likelihood = 1\n",
    "    for i in range(len(x)):\n",
    "        likelihood *= (sigmoid(x[i]) ** y[i]) * ((1 - sigmoid(x[i])) ** (1 - y[i]))\n",
    "    return likelihood\n",
    "\n",
    "\n",
    "def prior_like(w, variance):\n",
    "    prior = 1\n",
    "    for i in range(len(w)):\n",
    "        prior *= 1 / math.sqrt(2 * math.pi * variance) * math.exp(-(w[i] ** 2) / (2 * variance))\n",
    "    return prior\n",
    "\n",
    "\n",
    "def proposal_like(w, proposal_var, mean):\n",
    "    proposal = 1\n",
    "    for i in range(len(w)):\n",
    "        proposal *= 1 / math.sqrt(2 * math.pi * proposal_var) * math.exp(-((mean[i] - w[i]) ** 2) / (2 * proposal_var))\n",
    "    return proposal\n",
    "\n",
    "\n",
    "def r(x, y, w, prior_var, proposal_var, mean):\n",
    "    return likelihood(x, y) * prior_like(w, prior_var) / proposal_like(w, proposal_var, mean)\n",
    "\n",
    "\n",
    "def proposal(mean, variance):\n",
    "    return np.random.multivariate_normal(mean=mean, cov=np.eye(np.shape(mean)[0]) * variance)\n",
    "\n",
    "\n",
    "def sample_weights(sample_size, mean, variance):\n",
    "    w = list()\n",
    "    for i in range(sample_size):\n",
    "        w.append(proposal(mean, variance))\n",
    "    return w\n",
    "\n",
    "\n",
    "def compute_log_likelihood(y_pred, y):\n",
    "    log_p = np.dot(y.T, np.log(y_pred)) + np.dot(np.subtract(1, y).T, np.log(np.subtract(1, y_pred)))\n",
    "    return log_p\n",
    "\n",
    "def likelihood_2grad(X, x_prod):\n",
    "    hess = np.zeros((len(X[0]), len(X[0])))\n",
    "    sig_vec = np.multiply(sigmoid(x_prod), sigmoid(x_prod) - 1)\n",
    "    for i in range(len(x_prod)):\n",
    "        hess = np.add(hess, sig_vec[i] * np.outer(X[i], X[i].T))\n",
    "    return hess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Question 1 ---\n",
    "\n",
    "def laplace_approx(x_train, x_test, y_train, y_test, rate):\n",
    "\n",
    "    # variances\n",
    "    sigma_var = [0.5, 1, 2]\n",
    "\n",
    "    # acquire x matrices\n",
    "    X_train = X_mat(x_train)\n",
    "    X_test = X_mat(x_test)\n",
    "\n",
    "    # marginal likelihoods\n",
    "    marg_likelihoods = dict()\n",
    "\n",
    "    for variance in sigma_var:\n",
    "\n",
    "        # initialize weights and iteration count\n",
    "        w = np.zeros(np.shape(X_train[0]))\n",
    "        num_it = 0\n",
    "\n",
    "        # initialize first gradient\n",
    "\n",
    "        x_prod = np.reshape(np.dot(X_train, w), np.shape(y_train))\n",
    "        post_grad = likelihood_grad(X_train, x_prod, y_train) + prior_grad(w, variance)\n",
    "\n",
    "        # breakout when gradients become near zero\n",
    "        while max(post_grad) > 10**(-2):\n",
    "\n",
    "            # compute estimates, gradients, and update\n",
    "            x_prod = np.dot(X_train, w)\n",
    "            post_grad = likelihood_grad(X_train, x_prod, y_train) + prior_grad(w, variance)\n",
    "            w = np.add(w, rate*post_grad)\n",
    "\n",
    "            num_it += 1\n",
    "\n",
    "        # compute hessian at MAP solution\n",
    "        hessian = likelihood_2grad(X_train, x_prod) + prior_2grad(w, variance)\n",
    "\n",
    "        # compute marginal likelihood\n",
    "        print('marg lieklihood',log_likelihood(x_prod, y_train))\n",
    "        print(log_prior(w, variance))\n",
    "        print(log_g(hessian))\n",
    "        marg_likelihoods[variance] = log_likelihood(x_prod, y_train) + log_prior(w, variance) - log_g(hessian)\n",
    "\n",
    "        print('Number iterations for variance = ' + str(variance) + ' is ' + str(num_it))\n",
    "\n",
    "    return marg_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Results for Question 1 ---\n",
      "\n",
      "marg lieklihood [-67.47590821]\n",
      "-5.495777336635611\n",
      "1.8518962932848018\n",
      "Number iterations for variance = 0.5 is 1810\n",
      "marg lieklihood [-66.76486538]\n",
      "-6.408358059713926\n",
      "1.3328558628075875\n",
      "Number iterations for variance = 1 is 3047\n",
      "marg lieklihood [-66.30785946]\n",
      "-7.552090975517213\n",
      "0.9539697202443023\n",
      "Number iterations for variance = 2 is 4665\n",
      "For variance = 0.5 , with a marginal likelihood of [-74.82358184]\n",
      "For variance = 1 , with a marginal likelihood of [-74.50607931]\n",
      "For variance = 2 , with a marginal likelihood of [-74.81392016]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset('iris')\n",
    "y_train, y_valid, y_test = y_train[:, (1,)], y_valid[:, (1,)], y_test[:, (1,)]\n",
    "\n",
    "if True:\n",
    "    print('--- Results for Question 1 ---')\n",
    "    print('')\n",
    "    x_train, x_test = np.vstack((x_train, x_valid)), x_test\n",
    "    y_train, y_test = np.vstack((y_train, y_valid)), y_test\n",
    "    marginal_likelihood = laplace_approx(x_train, x_test, y_train, y_test, 0.001)\n",
    "    for var in marginal_likelihood:\n",
    "        print('For variance = ' + str(var) + ' , with a marginal likelihood of ' + str(marginal_likelihood[var]))\n",
    "    print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Question 2 ---\n",
    "\n",
    "def importance_sampling(x_train, x_valid, x_test, y_train, y_valid, y_test, mean, sample_range=[5, 10, 20, 50, 100, 500], visual=False):\n",
    "\n",
    "    # prior variance\n",
    "    prior_variance = 1\n",
    "\n",
    "    # proposal distribution metrics\n",
    "    variances = [1, 2, 5, 10]\n",
    "\n",
    "    # reconstruct y sets\n",
    "    y_train = np.asarray(y_train, int)\n",
    "    y_valid = np.asarray(y_valid, int)\n",
    "    y_test = np.asarray(y_test, int)\n",
    "\n",
    "    # make X matrices\n",
    "    X_train = X_mat(x_train)\n",
    "    X_valid = X_mat(x_valid)\n",
    "    X_test = X_mat(x_test)\n",
    "\n",
    "    min_ll = np.inf\n",
    "    for sample_size in sample_range:\n",
    "        for proposal_variance in variances:\n",
    "\n",
    "            # store all test set classifications\n",
    "            valid_pred = np.zeros(np.shape(y_valid))\n",
    "            valid_discrete_pred = np.zeros(np.shape(y_valid))\n",
    "\n",
    "            # sample s number of weights\n",
    "            w = sample_weights(sample_size, mean, proposal_variance)\n",
    "\n",
    "            # compute predictions over the whole validation set\n",
    "            for d in range(len(X_valid)):\n",
    "\n",
    "                # inner sample over j\n",
    "                r_sum = 0\n",
    "                for j in range(sample_size):\n",
    "                    # compute and sum r(w_js)\n",
    "                    r_sum += r(np.dot(X_train, w[j]), y_train, w[j], prior_variance, proposal_variance, mean)\n",
    "\n",
    "                # outer sample over i\n",
    "                pred_sum = 0\n",
    "                for i in range(sample_size):\n",
    "                    # compute sigmoid prediction of test point\n",
    "                    y_star = sigmoid(np.dot(X_valid[d], w[i]))\n",
    "                    # add to prediction summation \n",
    "                    pred_sum += y_star*r(np.dot(X_train, w[i]), y_train, w[i], prior_variance, proposal_variance, mean)/r_sum\n",
    "                \n",
    "                # make classification (discretized and continuous)\n",
    "                valid_pred[d] = pred_sum\n",
    "                if pred_sum > 0.5:\n",
    "                    valid_discrete_pred[d] = 1\n",
    "                elif pred_sum < 0.5:\n",
    "                    valid_discrete_pred[d] = 0\n",
    "                else:\n",
    "                    valid_discrete_pred[d] = -1\n",
    "\n",
    "            valid_log_likelihood = -compute_log_likelihood(valid_pred, y_valid)\n",
    "            if valid_log_likelihood < min_ll:\n",
    "                min_ll = valid_log_likelihood\n",
    "                min_acc = compute_accuracy_ratio(valid_discrete_pred, y_valid)\n",
    "                opt_var = proposal_variance\n",
    "                opt_ss = sample_size\n",
    "\n",
    "    # re-make X matrices with combined training and validation set\n",
    "    x_train = np.vstack((x_train, x_valid))\n",
    "    X_train = X_mat(x_train)\n",
    "    y_train = np.vstack((y_train, y_valid))\n",
    "\n",
    "    # store all test set classifications\n",
    "    test_pred = np.zeros(np.shape(y_test))\n",
    "    test_discrete_pred = np.zeros(np.shape(y_test))\n",
    "\n",
    "    # sample s number of weights\n",
    "    w = sample_weights(opt_ss, mean, opt_var)\n",
    "\n",
    "    # compute predictions over the whole test set\n",
    "    for d in range(len(X_test)):\n",
    "\n",
    "        # inner sample over j\n",
    "        r_sum = 0\n",
    "        for j in range(opt_ss):\n",
    "            # compute and sum r(w_js)\n",
    "            r_sum += r(np.dot(X_train, w[j]), y_train, w[j], prior_variance, opt_var, mean)\n",
    "\n",
    "        # outer sample over i\n",
    "        pred_sum = 0\n",
    "        for i in range(opt_ss):\n",
    "            # compute sigmoid prediction of test point\n",
    "            y_star = sigmoid(np.dot(X_test[d], w[i]))\n",
    "            # add to prediction summation \n",
    "            pred_sum += y_star*r(np.dot(X_train, w[i]), y_train, w[i], prior_variance, opt_var, mean)/r_sum\n",
    "\n",
    "        # make classification (discretized and continuous)\n",
    "        prediction = pred_sum\n",
    "        test_pred[d] = prediction\n",
    "        if prediction > 0.5:\n",
    "            test_discrete_pred[d] = 1\n",
    "        elif prediction < 0.5:\n",
    "            test_discrete_pred[d] = 0\n",
    "        else:\n",
    "            test_discrete_pred[d] = -1\n",
    "\n",
    "    test_accuracy_ratio = compute_accuracy_ratio(test_discrete_pred, y_test)\n",
    "    test_log_likelihood = compute_log_likelihood(test_pred, y_test)\n",
    "\n",
    "    if visual:\n",
    "        # sample s number of weights\n",
    "        w = sample_weights(5000, mean, 2)\n",
    "\n",
    "        # inner sample over j\n",
    "        r_sum = 0\n",
    "        for j in range(5000):\n",
    "            # compute and sum r(w_js)\n",
    "            r_sum += r(np.dot(X_train, w[j]), y_train, w[j], prior_variance, 2, mean)\n",
    "\n",
    "        # outer sample over i\n",
    "        posterior = list()\n",
    "        for i in range(5000):\n",
    "            # add to prediction summation\n",
    "            posterior.append(r(np.dot(X_train, w[i]), y_train, w[i], prior_variance, 2, mean) / r_sum)\n",
    "\n",
    "        visualize_posterior(mean, 2, posterior, w)\n",
    "\n",
    "    return -test_log_likelihood, test_accuracy_ratio, opt_var, opt_ss, min_ll, min_acc\n",
    "\n",
    "\n",
    "# --- Question 3 ---\n",
    "\n",
    "def generate_hastings_sample(X, y, sample_size, variance, map):\n",
    "    sample_means = list()\n",
    "    samples = list()\n",
    "\n",
    "    # must burned in 10000 iterations before storing sample\n",
    "    w_mean = map\n",
    "    w_i = proposal(w_mean, variance)\n",
    "    burned_in = False\n",
    "\n",
    "    while len(samples) < sample_size:\n",
    "\n",
    "        if not burned_in:\n",
    "            # burn in 10000 iterations\n",
    "            for j in range(1000):\n",
    "                # generate uniform random variable, and random sample\n",
    "                u = np.random.uniform()\n",
    "                w_star = proposal(w_i, variance)\n",
    "\n",
    "                if u < min(1, likelihood(np.dot(X, w_star), y) * prior_like(w_star, 1) / likelihood(np.dot(X, w_i),\n",
    "                                                                                             y) / prior_like(w_i, 1)):\n",
    "                    w_mean = w_i\n",
    "                    w_i = w_star\n",
    "\n",
    "            samples.append(w_i)\n",
    "            sample_means.append(w_mean)\n",
    "            burned_in = True\n",
    "            continue\n",
    "\n",
    "        for j in range(100):\n",
    "\n",
    "            # generate uniform random variable, and random sample\n",
    "            u = np.random.uniform()\n",
    "            w_star = proposal(w_i, variance)\n",
    "\n",
    "            if u < min(1, likelihood(np.dot(X, w_star), y) * prior_like(w_star, 1) / likelihood(np.dot(X, w_i),\n",
    "                                                                                             y) / prior_like(w_i, 1)):\n",
    "                w_mean = w_i\n",
    "                w_i = w_star\n",
    "\n",
    "        # collect sample every 100 iterations for thinning process (after 10000 burn in)\n",
    "        sample_means.append(w_mean)\n",
    "        samples.append(w_i)\n",
    "\n",
    "    r_sum = 0\n",
    "    for i in range(sample_size):\n",
    "        # compute and sum r(w_js)\n",
    "        r_sum += r(np.dot(X, samples[i]), y, samples[i], 1, variance, sample_means[i])\n",
    "\n",
    "    return samples, sample_means, r_sum\n",
    "\n",
    "\n",
    "def metropolis_hastings(x_train, x_valid, x_test, y_train, y_valid, y_test, map, visual=False):\n",
    "    # prior variance\n",
    "    prior_variance = 1\n",
    "\n",
    "    # sample size\n",
    "    sample_size = 100\n",
    "\n",
    "    # proposal distribution metrics\n",
    "    variances = [1, 2, 5, 10, 15]\n",
    "\n",
    "    # reconstruct y sets\n",
    "    y_train = np.asarray(y_train, int)\n",
    "    y_valid = np.asarray(y_valid, int)\n",
    "    y_test = np.asarray(y_test, int)\n",
    "\n",
    "    # make X matrices\n",
    "    X_train = X_mat(x_train)\n",
    "    X_valid = X_mat(x_valid)\n",
    "    X_test = X_mat(x_test)\n",
    "\n",
    "    min_ll = np.inf\n",
    "    for proposal_variance in variances:\n",
    "\n",
    "        # store all test set classifications\n",
    "        valid_pred = np.zeros(np.shape(y_valid))\n",
    "        valid_discrete_pred = np.zeros(np.shape(y_valid))\n",
    "\n",
    "        # sample s number of weights, and obtain r_sum\n",
    "        w, means, r_sum = generate_hastings_sample(X_train, y_train, sample_size, proposal_variance, map)\n",
    "\n",
    "        # compute predictions over the whole validation set\n",
    "        for d in range(len(X_valid)):\n",
    "\n",
    "            # outer sample over i\n",
    "            pred_sum = 0\n",
    "            for i in range(sample_size):\n",
    "                # compute sigmoid prediction of test point\n",
    "                y_star = sigmoid(np.dot(X_valid[d], w[i]))\n",
    "                # add to prediction summation\n",
    "                pred_sum += y_star * r(np.dot(X_train, w[i]), y_train, w[i], prior_variance, proposal_variance,\n",
    "                                       means[i]) / r_sum\n",
    "\n",
    "            # make classification (discretized and continuous)\n",
    "            valid_pred[d] = pred_sum\n",
    "            if pred_sum > 0.5:\n",
    "                valid_discrete_pred[d] = 1\n",
    "            elif pred_sum < 0.5:\n",
    "                valid_discrete_pred[d] = 0\n",
    "            else:\n",
    "                valid_discrete_pred[d] = -1\n",
    "\n",
    "        valid_log_likelihood = -compute_log_likelihood(valid_pred, y_valid)\n",
    "        if valid_log_likelihood < min_ll:\n",
    "            min_ll = valid_log_likelihood\n",
    "            min_acc = compute_accuracy_ratio(valid_discrete_pred, y_valid)\n",
    "            opt_var = proposal_variance\n",
    "\n",
    "    # re-make X matrices with combined training and validation set\n",
    "    x_train = np.vstack((x_train, x_valid))\n",
    "    X_train = X_mat(x_train)\n",
    "    y_train = np.vstack((y_train, y_valid))\n",
    "\n",
    "    # store all test set classifications\n",
    "    test_pred = np.zeros(np.shape(y_test))\n",
    "    test_discrete_pred = np.zeros(np.shape(y_test))\n",
    "\n",
    "    # sample s number of weights, and obtain r_sum\n",
    "    w, means, r_sum = generate_hastings_sample(X_train, y_train, sample_size, opt_var, map)\n",
    "\n",
    "    # store predictive posterior results for test point 9 and 10\n",
    "    ninth_pred = list()\n",
    "    tenth_pred = list()\n",
    "\n",
    "    # compute predictions over the whole test set\n",
    "    for d in range(len(X_test)):\n",
    "\n",
    "        # outer sample over i\n",
    "        pred_sum = 0\n",
    "        for i in range(sample_size):\n",
    "            # compute sigmoid prediction of test point\n",
    "            y_star = sigmoid(np.dot(X_test[d], w[i]))\n",
    "            if d == 9:\n",
    "                ninth_pred.append(y_star)\n",
    "            if d == 10:\n",
    "                tenth_pred.append(y_star)\n",
    "            # add to prediction summation\n",
    "            pred_sum += y_star * r(np.dot(X_train, w[i]), y_train, w[i], prior_variance, opt_var, means[i]) / r_sum\n",
    "\n",
    "        # make classification (discretized and continuous)\n",
    "        prediction = pred_sum\n",
    "        test_pred[d] = prediction\n",
    "        if prediction > 0.5:\n",
    "            test_discrete_pred[d] = 1\n",
    "        elif prediction < 0.5:\n",
    "            test_discrete_pred[d] = 0\n",
    "        else:\n",
    "            test_discrete_pred[d] = -1\n",
    "\n",
    "    test_accuracy_ratio = compute_accuracy_ratio(test_discrete_pred, y_test)\n",
    "    test_log_likelihood = compute_log_likelihood(test_pred, y_test)\n",
    "\n",
    "    if visual:\n",
    "        visualize_predictive_posterior([ninth_pred, tenth_pred])\n",
    "\n",
    "    return -test_log_likelihood, test_accuracy_ratio, opt_var, min_ll, min_acc\n",
    "\n",
    "\n",
    "# --- Visualization Functions ---\n",
    "\n",
    "def visualize_posterior(mean, variance, posterior, w):\n",
    "    # five components to each weight vector\n",
    "    for i in range(5):\n",
    "        weights = list()\n",
    "        # extract specific component of all weights\n",
    "        for j in range(len(w)):\n",
    "            weights.append(w[j][i])\n",
    "\n",
    "        # zip weights and posterior\n",
    "        weights, posterior = zip(*sorted(zip(weights, posterior)))\n",
    "\n",
    "        # set up gaussian\n",
    "        z = np.polyfit(weights, posterior, 1)\n",
    "        z = np.squeeze(z)\n",
    "        p = np.poly1d(z)\n",
    "\n",
    "        # plot\n",
    "        w_all = np.arange(min(weights), max(weights), 0.001)\n",
    "        q_w = scipy.stats.norm.pdf(w_all, mean[i], variance)\n",
    "        plt.figure(i)\n",
    "        plt.title(\"Posterior visualization: q(w) mean=\" + str(round(mean[i], 2)) + \" var=\" + str(variance))\n",
    "        plt.xlabel(\"w[\" + str(i) + \"]\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "        plt.plot(w_all, q_w, '-b', label=\"Proposal q(w)\")\n",
    "        plt.plot(weights, posterior, 'or', label=\"Posterior P(w|X,y)\")\n",
    "        plt.plot(weights, p(weights),\"r--\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig(\"weight_vis_\" + str(i) + \".png\")\n",
    "\n",
    "\n",
    "def visualize_predictive_posterior(predictive_posterior):\n",
    "    for i in range(len(predictive_posterior)):\n",
    "        plt.figure(i)\n",
    "        plt.title('Predictive Posterior for Flower ' + str(i+9))\n",
    "        plt.xlabel('Pr(y*|x*, w(i))')\n",
    "        plt.xlim((0, 1))\n",
    "        plt.ylabel('# Occurrences')\n",
    "        plt.hist(predictive_posterior[i], bins=20)\n",
    "        plt.savefig('flower_' + str(i+9) + '.png')\n",
    "\n",
    "\n",
    "# --- Main Block ---\n",
    "if __name__ == '__main__':\n",
    "    print('')\n",
    "    # Set True for Question 1\n",
    "    Q1 = False\n",
    "    # Set True for Question 2\n",
    "    Q2 = False\n",
    "    Q2_vis = False\n",
    "    # Set True for Question 3\n",
    "    Q3 = False\n",
    "    Q3_vis = False\n",
    "\n",
    "    # Import Dataset\n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset('iris')\n",
    "    y_train, y_valid, y_test = y_train[:, (1,)], y_valid[:, (1,)], y_test[:, (1,)]\n",
    "\n",
    "    # --- Question 1 ---\n",
    "    if Q1:\n",
    "        print('--- Results for Question 1 ---')\n",
    "        print('')\n",
    "        x_train, x_test = np.vstack((x_train, x_valid)), x_test\n",
    "        y_train, y_test = np.vstack((y_train, y_valid)), y_test\n",
    "        marginal_likelihood = laplace_approx(x_train, x_test, y_train, y_test, 0.001)\n",
    "        for var in marginal_likelihood:\n",
    "            print('For variance = ' + str(var) + ' , with a marginal likelihood of ' + str(marginal_likelihood[var]))\n",
    "        print('')\n",
    "\n",
    "    # --- Question 2 ---\n",
    "    if Q2:\n",
    "        print('--- Results for Question 2 ---')\n",
    "        print('')\n",
    "        map_mean = [-0.87805271, 0.29302957, -1.2347739, 0.67815586, -0.89401743]\n",
    "        if Q2_vis:\n",
    "            test_ll, test_ar, prop_var, ss, valid_ll, valid_ar = importance_sampling(x_train, x_valid, x_test, y_train,\n",
    "                                                                                     y_valid,\n",
    "                                                                                     y_test, map_mean, visual=True)\n",
    "        else:\n",
    "            test_ll, test_ar, prop_var, ss, valid_ll, valid_ar = importance_sampling(x_train, x_valid, x_test, y_train,\n",
    "                                                                                     y_valid, y_test, map_mean)\n",
    "        print('Proposal Distribution Variance: ' + str(prop_var))\n",
    "        print('Sample Size: ' + str(ss))\n",
    "        print('Validation Log-likelihood: ' + str(valid_ll))\n",
    "        print('Validation Accuracy Ratio: ' + str(valid_ar))\n",
    "        print('Test Log-likelihood: ' + str(test_ll))\n",
    "        print('Test Accuracy Ratio: ' + str(test_ar))\n",
    "        print('')\n",
    "        if Q2_vis:\n",
    "            print('See current directory for posterior visualization')\n",
    "        print('')\n",
    "\n",
    "    # --- Question 3 ---\n",
    "    if Q3:\n",
    "        print('--- Results for Question 3 ---')\n",
    "        print('')\n",
    "        map_mean = [-0.87805271, 0.29302957, -1.2347739, 0.67815586, -0.89401743]\n",
    "        if Q3_vis:\n",
    "            test_ll, test_ar, prop_var, valid_ll, valid_ar = metropolis_hastings(x_train, x_valid, x_test, y_train,\n",
    "                                                                                 y_valid, y_test, map_mean, visual=True)\n",
    "        else:\n",
    "            test_ll, test_ar, prop_var, valid_ll, valid_ar = metropolis_hastings(x_train, x_valid, x_test, y_train,\n",
    "                                                                                 y_valid, y_test, map_mean)\n",
    "        print('Proposal Distribution Variance: ' + str(prop_var))\n",
    "        print('Sample Size: ' + str(100))\n",
    "        print('Validation Log-likelihood: ' + str(valid_ll))\n",
    "        print('Validation Accuracy Ratio: ' + str(valid_ar))\n",
    "        print('Test Log-likelihood: ' + str(test_ll))\n",
    "        print('Test Accuracy Ratio: ' + str(test_ar))\n",
    "        print('')\n",
    "        if Q3_vis:\n",
    "            print('See current directory for flowers 9 and 10 predictive posterior histograms')\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
